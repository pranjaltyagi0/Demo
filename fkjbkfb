
import sqlite3
import pandas as pd
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Step 1: Load CSV into SQLite
def csv_to_sqlite(csv_file, db_file):
    # Load CSV into pandas DataFrame
    df = pd.read_csv(csv_file)
    
    # Connect to SQLite database (it will be created if it doesn't exist)
    conn = sqlite3.connect(db_file)
    
    # Convert the DataFrame to SQL table (replace if exists)
    df.to_sql('data_table', conn, if_exists='replace', index=False)
    
    # Commit and close the connection
    conn.commit()
    conn.close()
    
    print(f"Data from {csv_file} has been loaded into SQLite database {db_file}")

# Step 2: Query SQLite database
def query_sqlite(query, db_file):
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    
    # Execute the query and fetch results
    result = pd.read_sql_query(query, conn)
    
    # Close connection
    conn.close()
    
    return result

# Step 3: Retrieve data based on the user's query
def retrieve_data(query_text, db_file):
    # Adjust the column name based on your database schema
    sql_query = f"SELECT * FROM data_table WHERE column_name LIKE '%{query_text}%'"
    
    # Query the SQLite database
    data = query_sqlite(sql_query, db_file)
    
    return data

# Step 4: Augment the query with the retrieved data
def augment_data_with_retrieved_info(data, query_text):
    # Augment the response by appending the relevant data
    augmented_response = f"Query: {query_text}\nRetrieved Data:\n{data.to_string(index=False)}"
    return augmented_response

# Step 5: Generate response using local Hugging Face model
from transformers import GPT2LMHeadModel, GPT2Tokenizer

def generate_response(augmented_data):
    # Load the pre-trained GPT-2 model and tokenizer locally
    model_name = "gpt2"
    model = GPT2LMHeadModel.from_pretrained(model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    
    # Tokenize the input text and ensure the token length does not exceed the model's max length
    inputs = tokenizer.encode(augmented_data, return_tensors="pt", truncation=True, max_length=1024)
    
    # Generate text from the model
    # Set max_new_tokens instead of max_length to ensure we generate only the specified number of new tokens
    outputs = model.generate(inputs, max_new_tokens=200, num_return_sequences=1)
    
    # Decode and return the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_text

# Example Usage
augmented_data = "Query: What is the price of Product A?\nRetrieved Data:\nProduct A is $99.99"  # Example query and retrieved data
response = generate_response(augmented_data)
print("Generated Response:", response)


# Complete RAG System
def rag_system(query_text, csv_file, db_file):
    # Step 1: Load CSV into SQLite if it's not already loaded
    csv_to_sqlite(csv_file, db_file)
    
    # Step 2: Retrieve relevant data based on the query
    retrieved_data = retrieve_data(query_text, db_file)
    
    # Step 3: Augment the query with the retrieved data
    augmented_data = augment_data_with_retrieved_info(retrieved_data, query_text)
    
    # Step 4: Generate a response based on augmented data using Hugging Face model
    response = generate_response(augmented_data)
    
    return response

# Example Usage
csv_file = 'data.csv'  # Path to your CSV file
db_file = 'data.db'    # Path to your SQLite database

# Example query: asking for information related to a product
query = "What is the price of Product A?"

# Run the RAG system
response = rag_system(query, csv_file, db_file)

# Print the generated response
print("Generated Response:", response)




The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. 
Please pass your input's `attention_mask` to obtain reliable results.
Traceback (most recent call last):
  File "c:\Users\Pranjal.Tyagi1\Desktop\POC\main.py", line 187, in <module>
    response = rag_system(query, csv_file, db_file)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Pranjal.Tyagi1\Desktop\POC\main.py", line 175, in rag_system       
    response = generate_response(augmented_data)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\Pranjal.Tyagi1\Desktop\POC\main.py", line 157, in generate_response
    outputs = model.generate(inputs, max_length=200, num_return_sequences=1)        
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pranjal.Tyagi1\Desktop\POC\venv\Lib\site-packages\torch\utils\_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Pranjal.Tyagi1\Desktop\POC\venv\Lib\site-packages\transformers\generation\utils.py", line 2068, in generate
    self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)
  File "C:\Users\Pranjal.Tyagi1\Desktop\POC\venv\Lib\site-packages\transformers\generation\utils.py", line 1383, in _validate_generated_length
    raise ValueError(
ValueError: Input length of input_ids is 1024, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`
