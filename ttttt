import pandas as pd
import sqlite3

def csv_to_sqlite(csv_file, db_file):
    # Load CSV into pandas DataFrame
    df = pd.read_csv(csv_file)
    
    # Connect to SQLite database (it will be created if it doesn't exist)
    conn = sqlite3.connect(db_file)
    
    # Convert the DataFrame to SQL table (replace if exists)
    df.to_sql('data_table', conn, if_exists='replace', index=False)
    
    # Commit and close the connection
    conn.commit()
    conn.close()
    
    print(f"Data from {csv_file} has been loaded into SQLite database {db_file}")


def query_sqlite(query, db_file):
    # Connect to SQLite database
    conn = sqlite3.connect(db_file)
    
    # Execute the query and fetch results
    result = pd.read_sql_query(query, conn)
    
    # Close connection
    conn.close()
    
    return result


def retrieve_data(query_text, db_file):
    # Define a simple SQL template for querying based on user query
    # This could be improved with more sophisticated NLP-based query understanding
    sql_query = f"SELECT * FROM data_table WHERE column_name LIKE '%{query_text}%'"
    
    # Query the SQLite database
    data = query_sqlite(sql_query, db_file)
    
    return data

def augment_data_with_retrieved_info(data, query_text):
    # Augment the response by appending the relevant data
    augmented_response = f"Query: {query_text}\nRetrieved Data:\n{data.to_string(index=False)}"
    return augmented_response


from transformers import pipeline

def generate_response(augmented_data):
    # Load a text generation pipeline using Hugging Face's transformers
    generator = pipeline('text-generation', model='gpt2')  # You can choose other models like 'gpt-neo', 'gpt-3.5-turbo', etc.
    
    # Generate a response using the augmented data as input
    response = generator(augmented_data, max_length=200, num_return_sequences=1)
    
    # Extract the generated text from the response
    return response[0]['generated_text'].strip()



def rag_system(query_text, csv_file, db_file):
    # Step 1: Load CSV into SQLite if it's not already loaded
    csv_to_sqlite(csv_file, db_file)
    
    # Step 2: Retrieve relevant data based on the query
    retrieved_data = retrieve_data(query_text, db_file)
    
    # Step 3: Augment the query with the retrieved data
    augmented_data = augment_data_with_retrieved_info(retrieved_data, query_text)
    
    # Step 4: Generate a response based on augmented data using Hugging Face model
    response = generate_response(augmented_data)
    
    return response




csv_file = 'data.csv'  # Path to your CSV file
db_file = 'data.db'    # Path to your SQLite database

# Example query: asking for information related to a product
query = "What is the price of Product A?"

# Run the RAG system
response = rag_system(query, csv_file, db_file)

# Print the generated response
print("Generated Response:", response)

import os

from groq import Groq

client = Groq(
    api_key="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",
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Explain the importance of fast language models",
        }
    ],
    model="llama3-8b-8192",
)

print(chat_completion.choices[0].message.content)

    curl -X POST "https://api.groq.com/openai/v1/chat/completions" \
     -H "Authorization: Bearer $GROQ_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"messages": [{"role": "user", "content": "Explain the importance of fast language models"}], "model": "llama3-8b-8192"}'


"Final response structured rag": "You are provided with the tollva that tas buen, ex ated and returned the relevant data from the database. Mover's trigantint the ch user initially asked, which relates to the email data stored in the database.mour task is tortantergent Die Goxy Results: Understand the data returned from the MongoDB query. The data may contain information such as th addresses, subject, date, intent, and other email related fields. Relate it to the user's Question: use the context of the user's original question to identify the specific details they were looking for in the database response. Wornsulate a Final Answer: Based on the MongoDB query results and the user's question, construct a clear and concise response that addresses the user's query. Ensure the answer is easy to understand and provides the key information the user was searching for.\n\n User Query (query).\n Result generated from MongoDB (result).\n. The previous user chat history (chat_history).\n\n Your response should be clear and accurate. Avoid including any technical details such as internal queries or intermediate steps or from mongodb queries", "standalone_question": "You are an Assistant Bot, designed to reformulate the latest user question based on the user's

er.py

7

í”„

previous conversation.\n\AGUIDELINES: \n1. Make sure your response is informative and comprehensive.\n2. Make sure your response is clear and concise.\n3. Make sure your response is based solely on the information provided. \n\BINSTRUCTIONS: \nGiven the chat history and the latest user message, reformulate the user's input into a standalone question that can be understood without prior context. If the user's message refers to earlier conversation or lacks context, incorporate necessary information from the chat history to make it complete. DO NOT ANSWER the question; only provide the reformulated question. Do not include any additional text besides the question.\n\nRRCONTEXTER ACHAT HISTORY OF THE USER: (chat history)\BLATEST USER QUESTION: (question)\n\nYou can respond now.", question_intent": "Role and Primary Task: You are an advanced assistant Al with exceptional analytical and

" decision-making capabilities. Your primary task is to accurately interpret user queries related to their uploaded emails, determine the most appropriate action, and generate informative and relevant responses.\n\n\nGeneral Behavior:\n\n\nRespond to greetings warmly and briefly.\nIf asked about your identity or capabilities, explain concisely that you're an email assistant chatbot designed to help users with their queries related to emails. Inclassify user input query intent into one of these categories: salutation, normal_rag, summary_rag, structured_rag.\nStrict Decision Protocol:\n\n\nnormal_rag (DEFAULT CATEGORY):\n\n\nPurpose: Answering most questions using general information.\nuse when: The query can be answered by providing informative responses without requiring detailed unstructured information. \nAlways prioritize this category for most queries unless the query explicitly falls into another category. Anthis category also includes context-dependent follow-up questions like 'Tell me more about it' or 'Can you elaborate on that? "\nThis will be the first preferred RAG unless the query explicitly falls into another category.

\nsummary_rag:\n\n\nPurpose: Addressing questions about overall content, main ideas, or summaries of entire documents. Vnuse shen: The query explicitly requires a broad understanding or overview of a document's content as a whole. InExample

queries: \n'what is the main theme of this email?'\n'Summarize the key points of the entire email. "\n'Give me an overview

of the emails I've received. 'Instructuredrag:\n\n\nPurpose: Handling specific, detailed queries related to message

counts, dates, and recipients. Antise when: The query is about detailed information regarding enails metadata, such as

sent/received messages, senders, or date-specific filters.\nExample queries: \n' How many emails did I send on this date?
